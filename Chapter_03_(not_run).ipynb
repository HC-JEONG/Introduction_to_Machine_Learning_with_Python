{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 03 (not run).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HC-JEONG/Introduction_to_Machine_Learning_with_Python/blob/master/Chapter_03_(not_run).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2Pf4wpG0HnF",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 3 비지도 학습과 데이터 전처리\n",
        "\n",
        "비지도 학습이란 알고 있는 출력값이나 정보 없이 학습 알고리즘을 가르쳐야 하는 모든 종류의 머신러닝, 입력 데이터만으로 데이터에서 지식을 추출할 수 있어야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3on95j60Qu1",
        "colab_type": "text"
      },
      "source": [
        "# 3.1 비지도 학습의 종류\n",
        "\n",
        "\n",
        "> 비지도 변환(unsupervised transformation) : 데이터를 새롭게 표현하여 사람이나 다른 머신러닝 알고리즘이 원래 데이터보다 쉽게 해석할 수 있도록 만드는 알고리즘 - 차원축소(dimensionality reduction)\n",
        "\n",
        "\n",
        "> 군집(clustering) 알고리즘 : 데이터를 비슷한 것끼리 그룹으로 묶는 것."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3EIykUf0_ed",
        "colab_type": "text"
      },
      "source": [
        "# 3.2 비지도 학습의 도전 과제\n",
        "\n",
        "> 데이터를 잘 이해하고 싶을 때 탐색적 분석 단계에서 많이 사용한다.\n",
        "\n",
        "\n",
        "> 비지도 학습은 지도 학습의 전처리 단계에서도 사용된다.\n",
        "\n",
        "\n",
        "> 지도 학습 알고리즘에서 전처리와 스케일 조정을 자주 사용하지만, 스케일 조정 메서드는 지도 정보(supervised information)를 사용하지 않으므로 비지도 방식이다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfOgEUMF15DU",
        "colab_type": "text"
      },
      "source": [
        "# 3.3 데이터 전처리와 스케일 조정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4ZaBiee1-xC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from matplotlib import font_manager as fm\n",
        "from matplotlib import rc\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "!pip install mglearn\n",
        "import mglearn\n",
        "import scipy as sp\n",
        "import sklearn\n",
        "import graphviz\n",
        "\n",
        "#한글, 마이너스 기호 폰트 관련\n",
        "print(mpl.__version__)\n",
        "print(mpl.__file__)\n",
        "print(mpl.get_configdir())\n",
        "print(mpl.get_cachedir())\n",
        "\n",
        "#나눔 폰트 설치\n",
        "!apt-get update -qq\n",
        "!apt-get install fonts-nanum* -qq\n",
        "sys_font=fm.findSystemFonts()\n",
        "nanum_font = [f for f in sys_font if 'Nanum' in f]\n",
        "# 한번 프린트 해보자\n",
        "nanum_font\n",
        "path = '/usr/share/fonts/truetype/nanum/NanumGothicCoding.ttf'\n",
        "font_name = fm.FontProperties(fname=path, size=12).get_name()\n",
        "plt.rc('font', family=font_name)\n",
        "# 우선 fm._rebuild() 를 해주고\n",
        "fm._rebuild()\n",
        "mpl.rcParams['axes.unicode_minus'] = False # 마이너스 기호 오류 해결"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv_nceYP2Qkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mglearn.plots.plot_scaling()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3Q9RkHF3XvU",
        "colab_type": "text"
      },
      "source": [
        "##3.3.1 여러 가지 전처리 방법\n",
        "\n",
        "> StandardScaler : 각 특성의 평균을 0, 분산을 1로 변경, 특성의 최솟값과 최댓값 크기를 제한하지 않는다.\n",
        "\n",
        "\n",
        "> RobustScaler : 중간 값(median)과 사분위 값(quartile)을 사용, 전체 데이터와 아주 동떨어진 데이터 포인트(예를 들면, 측정 에러)(outlier)에 영향을 받지 않음.\n",
        "\n",
        "\n",
        "> MinMaxScaler는 모든 특성이 정확하게 0과 1사이에 위치하도록 데이터를 변경.\n",
        "\n",
        "\n",
        "> Normalizer는 특성 벡터의 유클리디안 길이가 1이 되도록 데이터 포인트를 조정. 각 데이터 포인트가 다른 비율로(길이에 반비례하여) 스케일이 조정된다는 뜻. 이러한 정규화(normalization)는 특성 벡터의 길이는 상관 없고 데이터의 방향 (또는 각도)만이 중요할 때 많이 사용.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STH2WPIi6cci",
        "colab_type": "text"
      },
      "source": [
        "## 3.3.2 데이터 변환 적용하기\n",
        "\n",
        "\n",
        "> 스케일을 조정하는 전처리 메서드들은 보통 지도 학습 알고리즘을 적용하기 전에 적용한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6Fjhmmj6sKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cancer dataset에 SVM(SVC)을 적용하고 데이터 전처리에는 MinMaxScaler를 사용\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "cancer=load_breast_cancer()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
        "                                                    random_state=1)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCBMkym3-w9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 전처리가 구현된 파이썬 클래스를 임포트하고 객체를 생성\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler=MinMaxScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7_B04j-BAdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scale 객체는 fit 메서드를 호출할 때 훈련 데이터(X_train)만 넘겨주며 y_train은 사용하지 않음.\n",
        "scaler.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-LAt_o0HVxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 실제로 훈련 데이터의 스케일을 조정하려면 스케일 객체의 transform 메서드를 사용\n",
        "\n",
        "# 데이터 변환\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "# 스케일이 조정된 후 데이터셋의 속성을 출력합니다.\n",
        "print(\"변환된 후 크기:\", X_train_scaled.shape)\n",
        "print(\"스케일 조정 전 특성별 최소값:\\n\", X_train.min(axis=0))\n",
        "print(\"스케일 조정 전 특성별 최대값:\\n\", X_train.max(axis=0))\n",
        "print(\"스케일 조정 후 특성별 최소값:\\n\", X_train_scaled.min(axis=0))\n",
        "print(\"스케일 조정 후 특성별 최대값:\\n\", X_train_scaled.max(axis=0))\n",
        "\n",
        "#모든 특성의 값은 0과 1 사이로 변환되고, 변환된 데이터의 배열 크기는 원래 데이터와 동일하다. 즉 특성 값이 이동되거나 크기가 조정되었을 뿐"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KwhV5yGIhXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 테스트 데이터 변환\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# 스케일이 조정된 후 테스트 데이터의 속성을 출력합니다.\n",
        "print(\"스케일 조정 후 특성별 최소값:\\n\", X_test_scaled.min(axis=0))\n",
        "print(\"스케일 조정 후 특성별 최대값:\\n\", X_test_scaled.max(axis=0))\n",
        "\n",
        "# 0~1 값이 아닌 이유는 훈련 세트와 테스트 세트에 같은 변환이 적용되어야 하기 때문에, transform 메서드는 테스트 세트에 훈련 세트의 최솟값을 빼고 훈련 세트의 범위로 나누기 때문이다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRshcB4QJTds",
        "colab_type": "text"
      },
      "source": [
        "## 3.3.3 (한국어판 부록) Quantile Transformer와 PowerTransformer\n",
        "\n",
        "\n",
        "> Quantile Transformer는 기본적으로 1,000개의 분위(quantile)을 사용하여 데이터를 균등하게 분포시킴. 이상치에 민감하지 않고 전체 데이터를 0과 1사이로 압축한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weFlDy0LJ9pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 필요한 모듈 임포트\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import QuantileTransformer, StandardScaler, PowerTransformer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls65bZ6_KLhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = make_blobs(n_samples=50, centers=2, random_state=4, cluster_std=1)\n",
        "X += 3\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, edgecolors='black')\n",
        "plt.xlim(0, 16)\n",
        "plt.xlabel('x0')\n",
        "plt.ylim(0, 10)\n",
        "plt.ylabel('x1')\n",
        "plt.title(\"Original Data\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzkWKAsJNbxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = QuantileTransformer()\n",
        "X_trans = scaler.fit_transform(X)\n",
        "\n",
        "plt.scatter(X_trans[:, 0], X_trans[:, 1], c=y, s=30, edgecolors='black')\n",
        "plt.xlim(0, 5)\n",
        "plt.xlabel('x0')\n",
        "plt.ylim(0, 5)\n",
        "plt.ylabel('x1')\n",
        "plt.title(type(scaler).__name__)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoZDO72zOLmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 히스토그램\n",
        "plt.hist(X_trans)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxnGmHsYOgzr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> QuantileTransformer의 분위 수는 n_quantiles 매개변수에서 설정할 수 있으며 기본값은 1,000이다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rmDttM3OoPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 속성 크기 확인\n",
        "print(scaler.quantiles_.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IelsnT0gO5pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 하나의 특성과 다섯 개의 샘플로 이루어진 간단한 데이터 셋\n",
        "x=np.array([[0], [5], [8], [9], [10]])\n",
        "\n",
        "# np.percentile() 함수는 두 번째 매개변수에서 지정한 분위에 해당하는 샘플을 추출하여 반환\n",
        "print(np.percentile(x[:, 0], [0, 25, 50, 75, 100]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLZmpEDbPaI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# QuantileTransformer로 변환 후 확인\n",
        "x_trans = QuantileTransformer().fit_transform(x)\n",
        "print(np.percentile(x_trans[:, 0], [0, 25, 50, 75, 100]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4gGjB4nP9PS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# QuantileTransformer의 output_distribution 매개변수에서 normal로 지정하여 정규분포로 출력을 바꿀 수 있다.\n",
        "scaler = QuantileTransformer(output_distribution='normal')\n",
        "X_trans = scaler.fit_transform(X)\n",
        "\n",
        "plt.scatter(X_trans[:, 0], X_trans[:, 1], c=y, s=30, edgecolors='black')\n",
        "plt.xlim(-5, 5)\n",
        "plt.xlabel('x0')\n",
        "plt.ylim(-5,5)\n",
        "plt.ylabel('x1')\n",
        "plt.title(type(scaler).__name__)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJJ602d2Qqyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(X)\n",
        "plt.title('Original Data')\n",
        "plt.show()\n",
        "\n",
        "X_trans = QuantileTransformer(output_distribution='normal').fit_transform(X)\n",
        "plt.hist(X_trans)\n",
        "plt.title('QuantileTransformer')\n",
        "plt.show()\n",
        "\n",
        "X_trans = StandardScaler().fit_transform(X)\n",
        "plt.hist(X_trans)\n",
        "plt.title('StandardScaler')\n",
        "plt.show()\n",
        "\n",
        "X_trans = PowerTransformer(method='box-cox').fit_transform(X)\n",
        "plt.hist(X_trans)\n",
        "plt.title('PowerTransformer box-cox')\n",
        "plt.show()\n",
        "\n",
        "X_trans = PowerTransformer(method='yeo-johnson').fit_transform(X)\n",
        "plt.hist(X_trans)\n",
        "plt.title('PowerTransformer yeo-johnson')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSvJpTUVR-CA",
        "colab_type": "text"
      },
      "source": [
        "## 3.3.4 훈련 데이터와 테스트 데이터의 스케일을 같은 방법으로 조정하기\n",
        "\n",
        "> 지도 학습 모델에서 테스트 세트를 사용하려면 훈련 세트와 테스트 세트에 같은 변환을 적용해야 한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR-KKwZzcZCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 스케일 조정 할 때 테스트 세트를 이용해서 하는 경우\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "# 인위적인 데이터 생성\n",
        "X, _= make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\n",
        "# 훈련 세트와 테스트 세트로 나눕니다.\n",
        "X_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n",
        "\n",
        "# 훈련 세트와 테스트 세트의 산점도를 그립니다.\n",
        "fig, axes = plt.subplots(1, 3, figsize=(13,4))\n",
        "axes[0].scatter(X_train[:, 0], X_train[:, 1],\n",
        "                c=mglearn.cm2.colors[0], label=\"훈련 세트\", s=60)\n",
        "axes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^',\n",
        "                c=mglearn.cm2.colors[1], label=\"테스트 세트\", s=60)\n",
        "axes[0].legend(loc='upper left')\n",
        "axes[0].set_title(\"원본 데이터\")\n",
        "\n",
        "#MinMaxScaler를 사용해 스케일을 조정합니다.\n",
        "scaler=MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 스케일이 조정된 데이터의 산점도를 그립니다.\n",
        "axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
        "                c=mglearn.cm2.colors[0], label=\"훈련 세트\", s=60)\n",
        "axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',\n",
        "                c=mglearn.cm2.colors[1], label=\"테스트 세트\", s=60)\n",
        "axes[1].set_title(\"스케일 조정된 데이터\")\n",
        "\n",
        "# 테스트 세트의 스케일을 따로 조정합니다.\n",
        "# 테스트 세트의 최솟값은 0, 최댓값은 1이 됩니다.\n",
        "# 이는 예제를 위한 것으로, 절대로 이렇게 사용해서는 안 됩니다.\n",
        "test_scaler=MinMaxScaler()\n",
        "test_scaler.fit(X_test)\n",
        "X_test_scaled_badly = test_scaler.transform(X_test)\n",
        "\n",
        "# 잘못 조정된 데이터의 산점도를 그립니다.\n",
        "axes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
        "                c=mglearn.cm2.colors[0], label=\"training set\", s=60)\n",
        "axes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],\n",
        "                marker='^', c=mglearn.cm2.colors[1], label=\"test set\", s=60)\n",
        "axes[2].set_title(\"잘못 조정된 데이터\")\n",
        "\n",
        "for ax in axes:\n",
        "  ax.set_xlabel(\"특성 0\")\n",
        "  ax.set_ylabel(\"특성 1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnmpIO3hA6TW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "# 메소드 체이닝(chaining)을 사용하여 fit과 transform을 연달아 호출합니다.\n",
        "X_scaled = scaler.fit(X_train).transform(X_train)\n",
        "# 위와 동일하지만 더 효율적입니다.\n",
        "X_scaled_d = scaler.fit_transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kkFLzc8BN1D",
        "colab_type": "text"
      },
      "source": [
        "## 3.3.5 지도 학습에서 데이터 전처리 효과"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNcXjjEaBNBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cancer dataset에서 SVC를 학습시킬 때 MinMaxScaler의 효과\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
        "                                                    random_state=0)\n",
        "\n",
        "svm=SVC(C=100)\n",
        "svm.fit(X_train, y_train)\n",
        "print(\"테스트 세트 정확도: {:.2f}\".format(svm.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2Av1mqTB2-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MinMaxScaler를 사용해 데이터 스케일 조정\n",
        "\n",
        "# 0~1 사이로 스케일 조정\n",
        "scaler=MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 조정된 데이터로 SVM 학습\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 스케일 조정된 테스트 세트의 정확도\n",
        "print(\"스케일 조정된 테스트 세트의 정확도: {:.2f}\".format(\n",
        "    svm.score(X_test_scaled, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjeQSUrtCfD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 평균 0, 분산 1을 갖도록 스케일 조정\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler=StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#조정된 데이터로 SVM 학습\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 스케일 조정된 테스트 세트의 정확도\n",
        "print(\"SVM test accuracy: {:.2f}\".format(svm.score(X_test_scaled, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5_8pJFzC9tW",
        "colab_type": "text"
      },
      "source": [
        "# 3.4 차원 축소, 특성 추출, 매니폴드 학습\n",
        "\n",
        "> 대체적으로 데이터를 시각화, 압축, 지도학습에 이용하기 위해 비지도 학습을 사용해 데이터를 변환한다.\n",
        "\n",
        "> 주성분 분석(principal component analysis, PCA), 비음수 행렬 분해(non-negative matrix factorization, NMF), t-SNE(t-distributed stochastic neighbor embedding) 등이 있다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfeErAUJDnU8",
        "colab_type": "text"
      },
      "source": [
        "## 3.4.1 주성분 분석(PCA)\n",
        "\n",
        "> 특성들이 통계적으로 상관관계가 없도록 데이터셋을 회전시키는 기술\n",
        "\n",
        "\n",
        "> 데이터를 회전한 뒤 새로운 특성 중 일부만 선택\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RSi6fSNDmDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mglearn.plots.plot_pca_illustration()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMDn0bO1FLN9",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> \"Component 1\"이라고 쓰여 있는, 분산이 가장 큰 방향을 찾는다.\n",
        "\n",
        "\n",
        "> 이 벡터가 가장 많은 정보를 담고 있는 방향이다. 즉, 특성들의 상관관계가 가장 큰 방향\n",
        "\n",
        "\n",
        "> 그 다음, Component 1과 직각인 방향 중에서 가장 많은 정보를 담은 방향을 찾는다. 이런 과정을 거쳐 찾은 방향을 데이터에 있는 주된 분산의 방향이라고 해서 주성분(principal component)라고 함.\n",
        "\n",
        "\n",
        "> 원본 특성 개수 = 주성분 개수\n",
        "\n",
        "> 두 번째 그래프는 Component1과 2를 각각 x, y 축에 나란하도록 회전한 것. PCA에 의해 회전된 두 축은 연관되어 있지 않으므로 변환된 데이터의 상관관계 행렬(correlation matrix)이 대각선 방향을 제외하고는 0이 됨.\n",
        "\n",
        "\n",
        "\n",
        "> PCA는 주성분의 일부만 남기는 차원 축소 용도로 사용할 수 있음. 세 번째 그래프는 첫 번째 주성분만 유지한 것. 이러면 2차원에서 1차원이 됨.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> PCA를 적용해 유방암 데이터셋 시각화하기\n",
        "\n",
        "\n",
        "> PCA가 가장 널리 사용되는 분야는 고차원 데이터셋의 시각화\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1Ifq22qG5zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(15, 2, figsize=(10,20))\n",
        "malignant = cancer.data[cancer.target == 0]\n",
        "benign = cancer.data[cancer.target ==1]\n",
        "\n",
        "ax= axes.ravel()\n",
        "\n",
        "for i in range(30):\n",
        "    _, bins = np.histogram(cancer.data[:, i], bins=50)\n",
        "    ax[i].hist(malignant[:, i], bins=bins, color=mglearn.cm3(0), alpha=.5)\n",
        "    ax[i].hist(benign[:, i], bins=bins, color=mglearn.cm3(2), alpha=.5)\n",
        "    ax[i].set_title(cancer.feature_names[i])\n",
        "    ax[i].set_yticks(())\n",
        "ax[0].set_xlabel(\"특성 크기\")\n",
        "ax[0].set_ylabel(\"빈도\")\n",
        "ax[0].legend([\"악성\", \"양성\"], loc=\"best\")\n",
        "fig.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EklhtYFKIJ9J",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> smoothness error는 두 히스토그램이 겹처 거의 쓸모 없음.\n",
        "\n",
        "\n",
        "> worst concave points는 두 히스토그램이 확실히 구분되어 유용한 특성.\n",
        "\n",
        "\n",
        "> 그러나 특성간의 상호작용은 위 히스토그램으로 알 수 없다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwYZhIMQIjuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 처음 두 개의 주성분을 찾아 2차원 공간에 하나의 산점도로 데이터 시각화\n",
        "# StandardScaler를 사용해 각 특성의 분산이 1이 되도록 데이터 스케일 조정\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(cancer.data)\n",
        "X_scaled = scaler.transform(cancer.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb36PS4gJGBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# 데이터의 처음 두 개 주성분만 유지시킵니다.\n",
        "pca = PCA(n_components=2)\n",
        "# 유방암 데이터로 PCA 모델을 만듭니다.\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# 처음 두 개의 주성분을 사용해 데이터를 변환시킵니다.\n",
        "X_pca = pca.transform(X_scaled)\n",
        "print(\"원본 데이터 형태:\", str(X_scaled.shape))\n",
        "print(\"축소된 데이터 형태:\", str(X_pca.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsHol0bbJc6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 클래스를 색깔로 구분하여 처음 두 개의 주성분을 그래프로 나타냅니다.\n",
        "plt.figure(figsize=(8, 8))\n",
        "mglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)\n",
        "plt.legend([\"악성\", \"양성\"], loc=\"best\")\n",
        "plt.gca().set_aspect(\"equal\")\n",
        "plt.xlabel(\"첫 번째 주성분\")\n",
        "plt.ylabel(\"두 번째 주성분\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d602dTmuKNTJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> PCA는 비지도 학습이기 때문에 데이터에 있는 상관관계만을 고려함.\n",
        "\n",
        "\n",
        "> PCA의 단점은 그래프의 두 축을 해석하기가 쉽지 않음. 원본 데이터가 조합된 형태이기 때문.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5Do-j_kKZzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"PCA 주성분 형태:\", pca.components_.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb_3ZZQrKheb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"PCA 주성분:\", pca.components_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1JMP_k_KnxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 히트맵으로 시각화\n",
        "plt.matshow(pca.components_, cmap='viridis')\n",
        "plt.yticks([0,1],[\"첫 번째 주성분\", \"두 번째 주성분\"])\n",
        "plt.colorbar()\n",
        "plt.xticks(range(len(cancer.feature_names)),\n",
        "           cancer.feature_names, rotation=60, ha='left')\n",
        "plt.xlabel(\"특성\")\n",
        "plt.ylabel(\"주성분\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P85x9NyLI-o",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> 고유얼굴(eigenface) 특성 추출\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S1nX6g_LVyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 실행 오래 걸림\n",
        "# PCA를 이용하여 LFW(Labeled Faces in the Wild) 데이터셋의 얼굴 이미지에서 특성 추출\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n",
        "image_shape=people.images[0].shape\n",
        "\n",
        "fig, axes=plt.subplots(2, 5, figsize=(15, 8),\n",
        "                      subplot_kw={'xticks': (), 'yticks': ()})\n",
        "for target, image, ax in zip(people.target, people.images, axes.ravel()):\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(people.target_names[target])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJWjHwmoM3F9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"people.images.shape:\", people.images.shape)\n",
        "print(\"클래스 개수:\", len(people.target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz_BMRO0NZ__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 각 타깃이 나타난 횟수 계산\n",
        "counts = np.bincount(people.target)\n",
        "# 타깃별 이름과 횟수 출력\n",
        "for i, (count, name) in enumerate(zip(counts, people.target_names)):\n",
        "    print(\"{0:25}{1:3}\".format(name, count), end='    ')\n",
        "    if (i+1) % 3 == 0:\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s95tvkEjN0mo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터셋의 편중을 없애기 위해 사람마다 50개의 이미지만 선택\n",
        "\n",
        "mask = np.zeros(people.target.shape, dtype=np.bool)\n",
        "for target in np.unique(people.target):\n",
        "    mask[np.where(people.target == target)[0][:50]]=1\n",
        "    \n",
        "X_people = people.data[mask]\n",
        "y_people = people.target[mask]\n",
        "\n",
        "# 0~255 사이의 흑백 이미지의 픽셀 값을 0~1 스케일로 조정합니다.\n",
        "# (옮긴이) MinMaxScaler를 적용하는 것과 거의 같습니다.\n",
        "X_people = X_people / 255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVyePcjhOidK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 분류하려는 얼굴과 가장 비슷한 얼굴 이미지를 찾는 1-최근접 이웃 분류기를 사용\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# 데이터를 훈련 세트와 테스트 세트로 나눕니다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_people, y_people, stratify=y_people, random_state=0)\n",
        "# 이웃 개수를 한 개로 하여 KNeighborsClassifier 모델을 만듭니다.\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "knn.fit(X_train, y_train)\n",
        "print(\"1-최근접 이웃의 테스트 세트 점수: {:.2f}\".format(knn.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVggoF4bPQBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mglearn.plots.plot_pca_whitening()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14wVogOWPYLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=100, whiten=True, random_state=0).fit(X_train)\n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "print(\"X_train_pca.shape:\", X_train_pca.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvwFf-TNPo1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "print(\"테스트 세트 정확도: {:.2f}\".format(knn.score(X_test_pca, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WWR7cRsQCNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"pca.components_.shape:\", pca.components_.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk6U9NK8QHiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(3, 5, figsize=(15, 12),\n",
        "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
        "for i, (component, ax) in enumerate(zip(pca.components_, axes.ravel())):\n",
        "    ax.imshow(component.reshape(image_shape), cmap='viridis')\n",
        "    ax.set_title(\"주성분 {}\".format((i+1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXwCv6U7Qvnj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> PCA 변환은 데이터를 회전시키고 분산이 작은 주성분을 덜어내는 것\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP1dT718QilM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mglearn.plots.plot_pca_faces(X_train, X_test, image_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKBINDN9R7dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mglearn.discrete_scatter(X_train_pca[:, 0], X_train_pca[:, 1], y_train)\n",
        "plt.xlabel(\"첫 번째 주성분\")\n",
        "plt.ylabel(\"두 번째 주성분\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ReLrmrvTdbi",
        "colab_type": "text"
      },
      "source": [
        "## 3.4.2비음수 행렬 분해(NMF)\n",
        "\n",
        "\n",
        "> NMF(non-negative matrix factorization)는 음수가 아닌 성분과 계수 값을 찾음. 즉, 주성분과 계수가 모두 0보다 크거나 같음.\n",
        "\n",
        "> NMF는 섞여 있는 데이터에서 원본 성분을 구분할 수 있음.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WPCFWs2UftS",
        "colab_type": "text"
      },
      "source": [
        "### 인위적 데이터에 NMF 적용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqlgbFe2S9UR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mglearn.plots.plot_nmf_illustration()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKBy1u8qVkSA",
        "colab_type": "text"
      },
      "source": [
        "### 얼굴 이미지에 NMF 적용하기\n",
        "\n",
        "\n",
        "> NMF의 핵심 매개변수는 추출할 성분의 갯수\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDC59oxMVtgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#실행 시간 오래 걸림\n",
        "mglearn.plots.plot_nmf_faces(X_train, X_test, image_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v15bjdLoX0Gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import NMF\n",
        "nmf = NMF(n_components=15, random_state=0) #성분 15개만 추출\n",
        "nmf.fit(X_train)\n",
        "X_train_nmf = nmf.transform(X_train)\n",
        "X_test_nmf = nmf.transform(X_test)\n",
        "fig, axes = plt.subplots(3, 5, figsize=(15, 12),\n",
        "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
        "for i, (component, ax) in enumerate(zip(nmf.components_, axes.ravel())):\n",
        "    ax.imshow(component.reshape(image_shape))\n",
        "    ax.set_title(\"성분 {}\".format(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9sa1oizZSEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compn = 3\n",
        "# 4번째 성분으로 정렬하여 처음 10개 이미지를 출력합니다\n",
        "inds = np.argsort(X_train_nmf[:, compn])[::-1]\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 8),\n",
        "                         subplot_kw={'xticks' : (), 'yticks': ()})\n",
        "for i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n",
        "    ax.imshow(X_train[ind].reshape(image_shape))\n",
        "    \n",
        "compn = 7\n",
        "# 8번째 성분으로 정렬하여 처음 10개 이미지를 출력합니다\n",
        "inds = np.argsort(X_train_nmf[:, compn])[::-1]\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 8),\n",
        "                         subplot_kw={'xticks':(), 'yticks':()})\n",
        "for i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n",
        "    ax.imshow(X_train[ind].reshape(image_shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqv1NXAGab8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 서로 다른 세 개의 입력으로부터 합성된 신호\n",
        "\n",
        "S = mglearn.datasets.make_signals()\n",
        "plt.figure(figsize=(6,1))\n",
        "plt.plot(S, '-')\n",
        "plt.xlabel(\"시간\")\n",
        "plt.ylabel(\"신호\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nksJJHOIa4pP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 원본 데이터를 사용해 100개의 측정 데이터를 만듭니다\n",
        "A = np.random.RandomState(0).uniform(size=(100,3))\n",
        "X = np.dot(S, A.T)\n",
        "print(\"측정 데이터 형태:\", X.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08qrJF2XbJSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NMF를 사용해 세 개의 신호를 복원\n",
        "nmf=NMF(n_components=3, random_state=42)\n",
        "S_=nmf.fit_transform(X)\n",
        "print(\"복원한 신호 데이터 형태:\", S_.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-bxZX-fbTcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PCA와 비교\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "H = pca.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7p1YoOlbaRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = [X, S, S_, H]\n",
        "names = ['측정 신호 (처음 3개)',\n",
        "         '원본 신호',\n",
        "         'NMF로 복원한 신호',\n",
        "         'PCA로 복원한 신호']\n",
        "\n",
        "fig, axes = plt.subplots(4, figsize=(8,4), gridspec_kw={'hspace': .5},\n",
        "                         subplot_kw={'xticks':(), 'yticks':()})\n",
        "\n",
        "for model, name, ax in zip(models, names, axes):\n",
        "    ax.set_title(name)\n",
        "    ax.plot(model[:, :3], '-')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvbBC2zMcQhZ",
        "colab_type": "text"
      },
      "source": [
        "##3.4.3 t-SNE를 이용한 매니폴드 학습\n",
        "\n",
        "\n",
        "> 매니폴드 학습(manifold learning)의 목적은 시각화.\n",
        "\n",
        "\n",
        "> 매니폴드 알고리즘은 훈련 데이터를 새로운 표현으로 변환시키지만 새로운 데이터에는 적용하지 못함. 즉 테스트 세트에는 적용할 수 없음.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0l_gMYndHRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 손글씨 숫자 데이터셋에 적용\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "digits=load_digits()\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10,5),\n",
        "                         subplot_kw={'xticks':(),'yticks':()})\n",
        "for ax, img in zip(axes.ravel(), digits.images):\n",
        "    ax.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhBApPtbdi12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PCA 모델을 생성합니다.\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(digits.data)\n",
        "#처음 두 개의 주성분으로 숫자 데이터를 변환합니다.\n",
        "digits_pca=pca.transform(digits.data)\n",
        "colors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\n",
        "          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\", \"#535D8E\"]\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.xlim(digits_pca[:,0].min(), digits_pca[:,0].max())\n",
        "plt.ylim(digits_pca[:,1].min(), digits_pca[:,1].max())\n",
        "for i in range(len(digits.data)):\n",
        "    # 숫자 텍스트를 이용해 산점도를 그립니다.\n",
        "    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\n",
        "            color=colors[digits.target[i]],\n",
        "            fontdict={'weight': 'bold', 'size':9})\n",
        "plt.xlabel(\"첫 번째 주성분\")\n",
        "plt.ylabel(\"두 번째 주성분\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgwVviX-e0Yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(random_state=42)\n",
        "# TSNE에는 transform 메서드가 없으므로 대신 fit_transform을 사용합니다.\n",
        "digits_tsne = tsne.fit_transform(digits.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VldUS3vzfF4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.xlim(digits_tsne[:,0].min(), digits_tsne[:,0].max()+1)\n",
        "plt.ylim(digits_tsne[:,1].min(), digits_tsne[:,1].max()+1)\n",
        "for i in range(len(digits.data)):\n",
        "    # 숫자 텍스트를 이용해 산점도를 그립니다.\n",
        "    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\n",
        "             color=colors[digits.target[i]],\n",
        "            fontdict={'weight': 'bold', 'size': 9})\n",
        "plt.xlabel(\"t-SNE 특성 0\")\n",
        "plt.ylabel(\"t-SNE 특성 1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE_dWKp_h-Qa",
        "colab_type": "text"
      },
      "source": [
        "# 3.5 군집\n",
        "\n",
        "\n",
        "> 군집(clustering)은 데이터셋을 클러스터(cluster)라는 그룹으로 나누는 작업. 한 클러스터 안의 데이터 포인트끼리는 매우 비슷하고 다른 클러스터의 데이터 포인트와는 구분되도록 데이터를 나누는 것이 목표\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWeceq19iPyh",
        "colab_type": "text"
      },
      "source": [
        "## 3.5.1 k-평균 군집\n",
        "\n",
        "> k-평균(k-means) 군집은 데이터의 어떤 영역을 대표하는 클러스터 중심(cluster center)를 찾음.\n",
        "\n",
        "> 데이터 포인트를 가장 가까운 클러스터 중심에 할당 -> 클러스터에 할당된 데이터 포인트의 평균으로 클러스터 중심을 다시 지정 -> 클러스터에 할당되는 데이터 포인트에 변화가 없을 때 알고리즘이 종료\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VR--cmCh97O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mglearn.plots.plot_kmeans_algorithm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-mielZji_dv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> 삼각형은 클러스터 중심이고 원은 데이터 포인트, 클러스터는 색으로 구분\n",
        "\n",
        "> 클러스터 중심으로 삼을 데이터를 무작위로 3개 선택(Initialization) -> 가장 가까운 클러스터 중심에 할당(Assign points(1)) -> 할당된 포인트의 평균값으로 클러스터 중심 갱신 -> 같은 작업 반복 후 클러스터 중심에 할당되는 포인트에 변화가 없으면 알고리즘 멈춤.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQWL5fuajg5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mglearn.plots.plot_kmeans_boundaries()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ATSqvKwjl9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 인위적으로 2차원 데이터를 생성합니다.\n",
        "X, y = make_blobs(random_state=1)\n",
        "\n",
        "# 군집 모델을 만듭니다.\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "kmeans.fit(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynUx-ryfkDA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"클러스터 레이블:\\n{}\".format(kmeans.labels_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESzTCzvSkPo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(kmeans.predict(X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnJ3CSZakg68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')\n",
        "mglearn.discrete_scatter(\n",
        "    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1],[0,1,2],\n",
        "    markers='^', markeredgewidth=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNdlIYx6mey4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes=plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# 두 개의 클러스터 중심을 사용합니다.\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "kmeans.fit(X)\n",
        "assignments=kmeans.labels_\n",
        "mglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[0])\n",
        "\n",
        "# 다섯 개의 클러스터 중심을 사용합니다.\n",
        "kmeans = KMeans(n_clusters = 5)\n",
        "kmeans.fit(X)\n",
        "assignments=kmeans.labels_\n",
        "\n",
        "mglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-Y5HlaOnOa1",
        "colab_type": "text"
      },
      "source": [
        "### k-평균 알고리즘이 실패하는 경우"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVJkNJZVpXOM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> 클러스터의 밀도가 다를 때 잘 처리하지 못함.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYsdbL-GnYZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_varied, y_varied = make_blobs(n_samples=200,\n",
        "                                cluster_std=[1.0, 2.5, 0.5],\n",
        "                                random_state=170)\n",
        "y_pred = KMeans(n_clusters=3, random_state=0).fit_predict(X_varied)\n",
        "mglearn.discrete_scatter(X_varied[:, 0], X_varied[:, 1], y_pred)\n",
        "plt.legend([\"클러스터 0\", \"클러스터 1\", \"클러스터 2\"], loc='best')\n",
        "plt.xlabel(\"특성 0\")\n",
        "plt.ylabel(\"특성 1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXDHUn68pSG6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> 원형이 아닌 클러스터를 잘 구분하지 못함\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4cF5VJDoSgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 무작위로 클러스터 데이터를 생성합니다.\n",
        "X, y = make_blobs(random_state=170, n_samples=600)\n",
        "rng = np.random.RandomState(74)\n",
        "\n",
        "# 데이터가 길게 늘어지도록 변경합니다.\n",
        "transformation = rng.normal(size=(2,2))\n",
        "X = np.dot(X, transformation)\n",
        "\n",
        "# 세 개의 클러스터로 데이터에 KMeans 알고리즘을 적용합니다.\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "kmeans.fit(X)\n",
        "y_pred=kmeans.predict(X)\n",
        "\n",
        "# 클러스터 할당과 클러스터 중심을 나타냅니다.\n",
        "mglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')\n",
        "mglearn.discrete_scatter(\n",
        "    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2],\n",
        "    markers='^', markeredgewidth=2)\n",
        "plt.xlabel(\"특성 0\")\n",
        "plt.ylabel(\"특성 1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUZbulbUpgLn",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> two-moons 데이터의 형태일때도 잘 구분하지 못함.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCZ_FHoKpfGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# two_moons 데이ㅓ를 생성합니다(이번에는 노이즈를 조금만 넣습니다).\n",
        "from sklearn.datasets import make_moons\n",
        "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
        "\n",
        "# 두 개의 클러스터로 데이터에 KMeans 알고리즘을 적용합니다.\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "kmeans.fit(X)\n",
        "y_pred = kmeans.predict(X)\n",
        "\n",
        "# 클러스터 할당과 클러스터 중심을 표시합니다.\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60, edgecolors='k')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            marker='^', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2,\n",
        "            edgecolors='k')\n",
        "plt.xlabel(\"특성 0\")\n",
        "plt.ylabel(\"특성 1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHnUggyBrIU0",
        "colab_type": "text"
      },
      "source": [
        "### 벡터 양자화 또는 분해 메서드로서의 k-평균\n",
        "\n",
        "> k-평균을 각 포인트가 하나의 성분으로 분해되는 관점으로 보는 것을 벡터 양자화(vector quantization)라고 한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC4BGiBXrcQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PCA, NMF, k-평균에서 추출한 성분과 100개의 성분으로 테스트 세트의 얼굴을 재구성한 것을 비교\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_people, y_people, stratify=y_people, random_state=42)\n",
        "nmf = NMF(n_components=100, random_state=0)\n",
        "nmf.fit(X_train)\n",
        "pca = PCA(n_components=100, random_state=0)\n",
        "pca.fit(X_train)\n",
        "kmeans = KMeans(n_clusters=100, random_state=0)\n",
        "kmeans.fit(X_train)\n",
        "\n",
        "X_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))\n",
        "X_reconstructed_kmeans = kmeans. cluster_centers_[kmeans.predict(X_test)]\n",
        "X_reconstructed_nmf = np.dot(nmf.transform(X_test), nmf.components_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSNF2dlyva2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes =plt.subplots(3, 5, figsize=(8, 8), subplot_kw={'xticks': (), 'yticks': ()})\n",
        "fig.suptitle(\"추출한 성분\")\n",
        "for ax, comp_kmeans, comp_pca, comp_nmf in zip(\n",
        "        axes.T, kmeans.cluster_centers_, pca.components_, nmf.components_):\n",
        "    ax[0].imshow(comp_kmeans.reshape(image_shape))\n",
        "    ax[1].imshow(comp_pca.reshape(image_shape), cmap='viridis')\n",
        "    ax[2].imshow(comp_nmf.reshape(image_shape))\n",
        "    \n",
        "axes[0,0].set_ylabel(\"kmeans\")\n",
        "axes[1,0].set_ylabel(\"pca\")\n",
        "axes[2,0].set_ylabel(\"nmf\")\n",
        "fig, axes = plt.subplots(4, 5, subplot_kw={'xticks': (), 'yticks': ()},\n",
        "                         figsize=(8,8))\n",
        "fig.suptitle(\"재구성\")\n",
        "for ax, orig, rec_kmeans, rec_pca, rec_nmf in zip(\n",
        "        axes.T, X_test, X_reconstructed_kmeans, X_reconstructed_pca,\n",
        "        X_reconstructed_nmf):\n",
        "    ax[0].imshow(orig.reshape(image_shape))\n",
        "    ax[1].imshow(rec_kmeans.reshape(image_shape))\n",
        "    ax[2].imshow(rec_pca.reshape(image_shape))\n",
        "    ax[3].imshow(rec_nmf.reshape(image_shape))\n",
        "    \n",
        "axes[0, 0].set_ylabel(\"원본\")\n",
        "axes[1, 0].set_ylabel(\"kmeans\")\n",
        "axes[2, 0].set_ylabel(\"pca\")\n",
        "axes[3, 0].set_ylabel(\"nmf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0Unj8w0FH-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
        "kmeans = KMeans(n_clusters=10, random_state=0)\n",
        "kmeans.fit(X)\n",
        "y_pred = kmeans.predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=60, cmap='Paired', edgecolors='black')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s=60,\n",
        "            marker='^', c=range(kmeans.n_clusters), linewidth=2, cmap='Paired',\n",
        "            edgecolors='black')\n",
        "plt.xlabel(\"특성 0\")\n",
        "plt.ylabel(\"특성 1\")\n",
        "print(\"클러스터 레이블:\\n\", y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6UKPQZlGO8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "distance_features = kmeans.transform(X)\n",
        "print(\"클러스터 거리 데이터의 형태:\", distance_features.shape)\n",
        "print(\"클러스터 거리:\\n\", distance_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyBWbIhiGery",
        "colab_type": "text"
      },
      "source": [
        "## 3.5.2 병합군집\n",
        "\n",
        "\n",
        "> 병합 군집(agglomerative clustering): 시작할 때 각 포인트를 하나의 클러스터로 지정하고, 그다음 어떤 종료 조건을 만족할 때까지 가장 비슷한 두 클러스터를 합쳐나가는 알고리즘. 종료조건은 클러스터 갯수\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45pMIL-gHzSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2차원 데이터셋에서 세 개의 클러스터를 찾기 위한 병합 군집의 과정\n",
        "mglearn.plots.plot_agglomerative_algorithm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAxMAt_sIUbc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> 병합 군집은 새로운 데이터 포인트에 대해서는 예측을 할 수 없다.\n",
        "\n",
        "> 대신 훈련 세트로 모델을 만들고 클러스터 소속 정보를 얻기 위해서 fit_predict 메서드를 사용.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAhMUGXTITIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "X, y= make_blobs(random_state=1)\n",
        "\n",
        "agg =AgglomerativeClustering(n_clusters=3)\n",
        "assignment = agg.fit_predict(X)\n",
        "\n",
        "mglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)\n",
        "plt.legend([\"클러스터 0\", \"클러스터 1\", \"클러스터 2\"], loc=\"best\")\n",
        "plt.xlabel(\"특성 0\")\n",
        "plt.ylabel(\"특성 1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LIhngE4NM9F",
        "colab_type": "text"
      },
      "source": [
        "### 계층적 군집과 덴드로그램\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm8XZcCRNX3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mglearn.plots.plot_agglomerative()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD5Rgk9rNfrG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> 계층 군집의 모습을 자세히 나타내지만, 2차원 데이터일 때 뿐이며, 특성이 셋 이상인 데이터셋에는 사용할 수 없다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hroeCTgeNo2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SciPy에서 ward 군집 함수와 덴드로그램 함수를 임포트합니다.\n",
        "from scipy.cluster.hierarchy import dendrogram, ward\n",
        "X, y= make_blobs(random_state=0, n_samples=12)\n",
        "# 데이터 배열 X에 ward 함수를 적용합니다.\n",
        "# SciPy의 ward 함수는 병합 군집을 수행할 때 생성된\n",
        "# 거리 정보가 담긴 배열을 반환합니다.\n",
        "linkage_array = ward(X)\n",
        "# 클러스터 간의 거리 정보가 담긴 linkage_array를 사용해 덴드로그램을 그립니다.\n",
        "dendrogram(linkage_array)\n",
        "\n",
        "# 두 개와 세 개의 클러스터를 구분하는 커트라인을 표시합니다.\n",
        "ax = plt.gca()\n",
        "bounds = ax.get_xbound()\n",
        "ax.plot(bounds, [7.25, 7.25], '--', c='k')\n",
        "ax.plot(bounds, [4,4], '--', c='k')\n",
        "\n",
        "ax.text(bounds[1], 7.25, ' 두 개 클러스터', va='center', fontdict={'size': 15})\n",
        "ax.text(bounds[1], 4, ' 세 개 클러스터', va='center', fontdict={'size': 15})\n",
        "plt.xlabel(\"샘플 번호\")\n",
        "plt.ylabel(\"클러스터 거리\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN80e3rvO8BK",
        "colab_type": "text"
      },
      "source": [
        "## 3.5.3 DBSCAN\n",
        "\n",
        "\n",
        "> DBSCAN(density-based spatial clustering of applications with noise)\n",
        "\n",
        "\n",
        "> 클러스터의 개수를 미리 지정할 필요가 없음.\n",
        "\n",
        "\n",
        "> 어떤 클래스에도 속하지 않는 포인트를 구분할 수 있음.\n",
        "\n",
        "\n",
        "> 밀집 지역(dense region) : 데이터가 많아 붐비는 지역\n",
        "\n",
        "> 핵심 샘플 : 밀집 지역에 있는 포인트\n",
        "\n",
        "\n",
        "> DBSCAN의 두 개의 매개변수 min_samples, eps\n",
        "\n",
        "> eps 거리 안에 데이터가 min_samples 개수만큼 들어 있으면 이 데이터 포인트를 핵심 샘플로 분류.\n",
        "\n",
        "> eps보다 가까운 핵심 샘플은 DBSCAN에 의해 동일한 클러스터로 합쳐짐.\n",
        "\n",
        "> eps 거리 안에 있는 포인트 수가 min_samples보다 적다면 그 포인트는 어떤 클래스에도 속하지 않는 잡음(noise)으로 레이블.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVWC3YWwQUrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "X, y = make_blobs(random_state=0, n_samples=12)\n",
        "\n",
        "dbscan=DBSCAN()\n",
        "clusters= dbscan.fit_predict(X)\n",
        "print(\"클러스터 레이블:\\n\", clusters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVwJ8y9MQmku",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> -1은 잡음 포인트를 의미\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ID3_xBzQtLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mglearn.plots.plot_dbscan()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NohpVRiSQ6jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# two_moons 데이터셋에 DBSCAN을 적용한 결과\n",
        "\n",
        "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
        "\n",
        "# 평균이 0, 분산이 1이 되도록 데이터의 스케일을 조정합니다.\n",
        "scaler=StandardScaler()\n",
        "scaler.fit(X)\n",
        "X_scaled = scaler.transform(X)\n",
        "\n",
        "dbscan = DBSCAN()\n",
        "clusters=dbscan.fit_predict(X_scaled)\n",
        "# 클러스터 할당을 표시합니다.\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm2, s=60,\n",
        "            edgecolors='black')\n",
        "plt.xlabel(\"특성 0\")\n",
        "plt.ylabel(\"특성 1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnA0Aw6dRrOl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> DBSCAN을 사용할 때 클러스터 할당 값(eps)을 주의해서 다뤄야 함.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVZa8MNHRzDh",
        "colab_type": "text"
      },
      "source": [
        "## 3.5.4 군집 알고리즘의 비교와 평가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7OJrCwmR39p",
        "colab_type": "text"
      },
      "source": [
        "### 타깃값으로 군집 평가하기\n",
        "\n",
        "\n",
        "> 군집 알고리즘의 결과를 실제 정답 클러스터와 비교하여 평가할 수 있는 지표로 ARI(adjusted rand index)와 NMI(normalized mutual information)가 있다.\n",
        "\n",
        "> 1 : 최적일 때, 0 : 무작위로 분류될 때. (ARI는 음수가 될 수 있다.)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQbW5dbiSjGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ARI를 사용해서 k-평균, 병합 군집, DBSCAN 알고리즘 비교\n",
        "\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
        "\n",
        "# 평균이 0, 분산이 1이 되도록 데이터의 스케일을 조정합니다.\n",
        "scaler=StandardScaler()\n",
        "scaler.fit(X)\n",
        "X_scaled = scaler.transform(X)\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(15, 3),\n",
        "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
        "\n",
        "# 사용할 알고리즘 모델을 리스트로 만듭니다.\n",
        "algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n",
        "              DBSCAN()]\n",
        "\n",
        "# 비교를 위해 무작위로 클러스터 할당합니다.\n",
        "random_state = np.random.RandomState(seed=0)\n",
        "random_clusters=random_state.randint(low=0, high=2, size=len(X))\n",
        "\n",
        "# 무작위 할당한 클러스터를 그립니다.\n",
        "axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n",
        "                cmap=mglearn.cm3, s=60, edgecolors='black')\n",
        "axes[0].set_title(\"무작위 할당 - ARI: {:.2f}\".format(\n",
        "        adjusted_rand_score(y, random_clusters)))\n",
        "\n",
        "for ax, algorithm in zip(axes[1:], algorithms):\n",
        "    # 클러스터 할당과 클러스터 중심을 그립니다.\n",
        "    clusters = algorithm.fit_predict(X_scaled)\n",
        "    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters,\n",
        "               cmap=mglearn.cm3, s=60, edgecolors='black')\n",
        "    ax.set_title(\"{} - ARI: {:.2f}\".format(algorithm.__class__.__name__,\n",
        "                                           adjusted_rand_score(y, clusters)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLGwa8M-U9aD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> 군집 모델을 평가할 때는 adjusted_rand_score, normalized_mutual_info_score 같은 군집용 측정 도구를 사용해야 한다.\n",
        "\n",
        "\n",
        "> accuracy_score를 사용하면 레이블 이름이 일치하는지 확인하는 것이므로 군집 모델 평가에는 부적합하다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm2oeZKKVR4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 포인트가 클러스터로 나뉜 두 가지 경우\n",
        "clusters1= [0,0,1,1,0]\n",
        "clusters2=[1,1,0,0,1]\n",
        "# 모든 레이블이 달라졌으므로 정확도가 0입니다.\n",
        "print(\"정확도: {:.2f}\".format(accuracy_score(clusters1, clusters2)))\n",
        "# 같은 포인트가 클러스터에 모였으므로 ARI는 1입니다.\n",
        "print(\"ARI: {:.2f}\".format(adjusted_rand_score(clusters1, clusters2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT6sFSlgV1FB",
        "colab_type": "text"
      },
      "source": [
        "### 타깃값 없이 군집 평가하기\n",
        "\n",
        "> 실루엣 계수(silhouette coefficient) : 타깃값이 필요 없는 군집용 지표\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_stTTBC2WZEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.cluster import silhouette_score\n",
        "\n",
        "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
        "\n",
        "# 평균이 0, 분산이 1이 되도록 데이터의 스케일을 조정합니다.\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X_scaled = scaler.transform(X)\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(15, 3),\n",
        "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
        "\n",
        "# 비교를 위해 무작위로 클러스터 할당합니다.\n",
        "random_state = np.random.RandomState(seed=0)\n",
        "random_clusters = random_state.randint(low=0, high=2, size=len(X))\n",
        "\n",
        "# 무작위 할당한 클러스터를 그립니다.\n",
        "axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n",
        "                cmap=mglearn.cm3, s=60, edgecolors='black')\n",
        "axes[0].set_title(\"무작위 할당: {:.2f}\".format(\n",
        "        silhouette_score(X_scaled, random_clusters)))\n",
        "\n",
        "algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2), DBSCAN()]\n",
        "\n",
        "for ax, algorithm in zip(axes[1:], algorithms):\n",
        "    clusters = algorithm.fit_predict(X_scaled)\n",
        "    # 클러스터 할당과 클러스터 중심을 그립니다.\n",
        "    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm3,\n",
        "               s=60, edgecolors='black')\n",
        "    ax.set_title(\"{} : {:.2f}\".format(algorithm.__class__.__name__,\n",
        "                                      silhouette_score(X_scaled, clusters)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW3jDmwwYLRr",
        "colab_type": "text"
      },
      "source": [
        "### 얼굴 데이터셋으로 군집 알고리즘 비교"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOP2Tqxhen4G",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> DBSCAN으로 얼굴 데이터셋 분석하기\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1GutnPMYQ4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LFW 데이터에서 고유얼굴을 찾은 다음 데이터를 변환합니다.\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=100, whiten=True, random_state=0)\n",
        "X_pca = pca.fit_transform(X_people)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyDFbONhbGFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DBSCAN으로 얼굴 데이터셋 분석하기\n",
        "\n",
        "# 기본 매개변수로 DBSCAN을 적용합니다.\n",
        "dbscan = DBSCAN()\n",
        "labels = dbscan.fit_predict(X_pca)\n",
        "print(\"고유한 레이블:\", np.unique(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWZOxze1bYSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dbscan = DBSCAN(min_samples=3)\n",
        "labels = dbscan.fit_predict(X_pca)\n",
        "print(\"고유한 레이블:\", np.unique(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYgMcOXFbgus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dbscan = DBSCAN(min_samples=3, eps=15)\n",
        "labels = dbscan.fit_predict(X_pca)\n",
        "print(\"고유한 레이블:\", np.unique(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKaZK4dgbz5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 잡음 포인트와 클러스터에 속한 포인트 수를 셉니다.\n",
        "# bincount는 음수를 받을 수 없어서 labels에 1을 더했습니다.\n",
        "# 반환값의 첫 번째 원소는 잡음 포인트의 수입니다.\n",
        "print(\"클러스터별 포인트 수:\", np.bincount(labels + 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCpForiecFlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noise = X_people[labels==-1]\n",
        "\n",
        "fig, axes = plt.subplots(3, 9, subplot_kw={'xticks': (), 'yticks': ()},\n",
        "                         figsize=(12,4))\n",
        "for image, ax in zip(noise, axes.ravel()):\n",
        "    ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZqCr59Pcigk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> 이상치 검출(outlier detection): 특이한 것을 찾아내는 분석\n",
        "\n",
        "> 큰 클러스터 하나보다 더 많은 클러스터를 찾으려면 eps를 0.5(기본값)~15 사이 정도로 줄여야 함.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzsoKlBfc2N4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for eps in [1,3,5,7,9,11,13]:\n",
        "    print(\"\\neps=\", eps)\n",
        "    dbscan=DBSCAN(eps=eps, min_samples=3)\n",
        "    labels=dbscan.fit_predict(X_pca)\n",
        "    print(\"클러스터 수:\", len(np.unique(labels)))\n",
        "    print(\"클러스터 크기:\", np.bincount(labels+1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjruFTwpdSUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 사진 안뜨는 오류남\n",
        "dbscan = DBSCAN(min_samples=3, eps=7)\n",
        "labels = dbscan.fit_predict(X_pca)\n",
        "\n",
        "for cluster in range(max(labels)+1):\n",
        "    mask = labels == clusters\n",
        "    n_images = np.sum(mask)\n",
        "    fig, axes = plt.subplots(1, 14, figsize=(14*1.5, 4),\n",
        "                             subplot_kw={'xticks': (), 'yticks': ()})\n",
        "    i = 0\n",
        "    for image, label, ax in zip(X_people[mask], y_people[mask], axes):\n",
        "        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n",
        "        ax.set_title(people.target_names[label].split()[-1])\n",
        "        i +=1\n",
        "    for j in range(len(axes) -i):\n",
        "        axes[j+i].imshow(np.array([[1]*65]*87), vmin=0, vmax=1)\n",
        "        axes[j+i].axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TKX0680eu-F",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> k-평균으로 얼굴 데이터셋 분석하기\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umXFP0Z1eyzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# k-평균으로 클러스터를 추출합니다\n",
        "km = KMeans(n_clusters=10, random_state=0)\n",
        "labels_km = km.fit_predict(X_pca)\n",
        "print(\"k-평균의 클러스터 크기:\", np.bincount(labels_km))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh3OOJYcfwOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(2, 5, subplot_kw={'xticks': (), 'yticks': ()},\n",
        "                         figsize=(12,4))\n",
        "for center, ax in zip(km.cluster_centers_, axes.ravel()):\n",
        "    ax.imshow(pca.inverse_transform(center).reshape(image_shape),\n",
        "              vmin=0, vmax=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZtPhxA3jAH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mglearn.plots.plot_kmeans_faces(km, pca, X_pca, X_people,\n",
        "                                y_people, people.target_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4AUfrGmjUgK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> 병합 군집으로 얼굴 데이터셋 분석하기\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmyw1o0ajXUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 병합 군집으로 클러스터를 추출합니다\n",
        "agglomerative = AgglomerativeClustering(n_clusters=10)\n",
        "labels_agg = agglomerative.fit_predict(X_pca)\n",
        "print(\"병합 군집의 클러스터 크기:\", np.bincount(labels_agg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KezIwvRsjpth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ARI 점수를 이용해 병합 군집과 k-평균으로 만든 두 데이터가 비슷한지 측정\n",
        "print(\"ARI: {:.2f}\".format(adjusted_rand_score(labels_agg, labels_km)))\n",
        "# 0.09는 두 군집 사이에 공통 부분이 거의 없다는 것"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvQ1iIfVj93c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 덴드로그램\n",
        "linkage_array = ward(X_pca)\n",
        "# 클러스터 사이의 거리가 담겨 있는 linkage_array로 덴드로그램을 그립니다.\n",
        "plt.figure(figsize=(20,5))\n",
        "dendrogram(linkage_array, p=7, truncate_mode='level', no_labels=True)\n",
        "plt.xlabel(\"샘플 번호\")\n",
        "plt.ylabel(\"클러스터 거리\")\n",
        "ax = plt.gca()\n",
        "bounds = ax.get_xbound()\n",
        "ax.plot(bounds, [36, 36], '--', c='k')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSkHgltCkbUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_clusters = 10\n",
        "for cluster in range(n_clusters):\n",
        "    mask = labels_agg == cluster\n",
        "    fig, axes = plt.subplots(1, 10, subplot_kw={'xticks': (), 'yticks': ()},\n",
        "                             figsize=(15,8))\n",
        "    axes[0].set_ylabel(np.sum(mask))\n",
        "    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n",
        "                                      labels_agg[mask], axes):\n",
        "        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n",
        "        ax.set_title(people.target_names[label].split()[-1],\n",
        "                     fontdict={'fontsize':9})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb3A9WRdlHwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 병합 군집으로 클러스터를 추출합니다.\n",
        "agglomerative = AgglomerativeClustering(n_clusters=40)\n",
        "labels_agg = agglomerative.fit_predict(X_pca)\n",
        "print(\"병합 군집의 클러스터 크기:\", np.bincount(labels_agg))\n",
        "\n",
        "n_clusters = 40\n",
        "for cluster in [13, 16, 23, 38, 39]: # 흥미로운 클러스터 몇개를 골랐습니다.\n",
        "    mask = labels_agg == cluster\n",
        "    fig, axes = plt.subplots(1, 15, subplot_kw={'xticks': (), 'yticks': ()},\n",
        "                             figsize=(15,8))\n",
        "    cluster_size = np.sum(mask)\n",
        "    axes[0].set_ylabel(\"#{}: {}\".format(cluster, cluster_size))\n",
        "    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n",
        "                                      labels_agg[mask], axes):\n",
        "        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n",
        "        ax.set_title(people.target_names[label].split()[-1],\n",
        "                     fontdict={'fontsize':9})\n",
        "    for i in range(cluster_size, 15):\n",
        "        axes[i].set_visible(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vndqpxRtmpW6",
        "colab_type": "text"
      },
      "source": [
        "## 3.5.5 군집 알고리즘 요약\n",
        "\n",
        "\n",
        "> k-평균과 병합 군집은 원하는 클러스터 개수를 지정할 수 있다.\n",
        "\n",
        "\n",
        "> DBSCAN은 eps 매개변수를 사용하여 클러스터 크기를 간접적으로 조절할 수 있다.\n",
        "\n",
        "> k-평균은 클러스터 중심을 사용해 클러스터를 구분\n",
        "\n",
        "\n",
        "> DBSCAN은 클러스터에 할당되지 않는 잡음 포인트를 인식할 수 있고 클러스터의 개수를 자동으로 결정한다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dblz6hNoGz4",
        "colab_type": "text"
      },
      "source": [
        "## 3.6 요약 및 정리\n",
        "\n",
        "> 분해, 매니폴드 학습, 군집은 주어진 데이터에 대한 이해를 높이기 위한 필수 도구이며, 레이블 정보가 없을 때 데이터를 분석할 수 있는 유일한 방법이다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6hCnBVJofr4",
        "colab_type": "text"
      },
      "source": [
        "### 추정기 인터페이스 요약\n",
        "\n",
        "> scikit-learn의 모든 전처리, 지도 학습, 비지도 학습 알고리즘은 파이썬 클래스로 구현되어 있고, 이 파이썬 클래스를 추정기(estimator)라고 부름.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Z3q3Wloxwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#알고리즘을 사용하려면 먼저 이 파이썬 클래스의 객체를 생성해야 한다.\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "logreg=LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cBZ8K2KpTa8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> 모든 추정기 클래스는 모델을 만들 때 사용하는 fit 메서드를 제송한다. fit 메서드는 항상 첫 번째 매개변수로 데이터 X를 필요로 하며, 하나의 데이터 포인트가 하나의 행이고 연속된 실숫값으로 표현된 NumPy 배열이거나 SciPy 희소 행렬이다.\n",
        "\n",
        "> 지도 학습 알고리즘은 회귀나 분류에서 필요한 타깃값을 가지고 있는 1차원 NumPy 배열인 y 매개변수도 필요로 한다.\n",
        "\n",
        "> y 배열과 같은 형태로 새 예측을 만들기 위해서는 predict 메서드를 사용\n",
        "\n",
        "\n",
        "> 입력 데이터 X의 새로운 표현 형태를 얻기 위해서는 predict 메서드를 사용\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}